#!/usr/bin/env python3
"""
Comprehensive Test Validation Script for Enhanced AI Agent
Tests the new LLM fine-tuning and application testing features.
"""

import os
import sys
import json
import time
import requests
import subprocess
from datetime import datetime

# Add the src directory to the Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

def test_backend_health():
    """Test if the backend is running and healthy."""
    print("üîç Testing backend health...")
    try:
        response = requests.get('http://localhost:5001/health', timeout=5)
        if response.status_code == 200:
            data = response.json()
            print(f"‚úÖ Backend is healthy (version {data.get('version', 'unknown')})")
            print(f"   Modules: {data.get('modules', {})}")
            return True
        else:
            print(f"‚ùå Backend health check failed: {response.status_code}")
            return False
    except Exception as e:
        print(f"‚ùå Backend health check failed: {e}")
        return False

def test_capabilities_endpoint():
    """Test the capabilities endpoint."""
    print("\nüîç Testing capabilities endpoint...")
    try:
        response = requests.get('http://localhost:5001/api/capabilities', timeout=5)
        if response.status_code == 200:
            data = response.json()
            
            # Check for new capabilities
            required_capabilities = ['llm_finetuning', 'app_testing']
            for capability in required_capabilities:
                if capability in data:
                    print(f"‚úÖ {capability} capability found")
                else:
                    print(f"‚ùå {capability} capability missing")
                    return False
            
            # Check LLM fine-tuning features
            llm_features = data.get('llm_finetuning', {}).get('features', [])
            if 'Custom Training' in llm_features:
                print("‚úÖ LLM fine-tuning features properly configured")
            else:
                print("‚ùå LLM fine-tuning features missing")
                return False
            
            # Check app testing features
            testing_types = data.get('app_testing', {}).get('types', [])
            if 'Unit Tests' in testing_types and 'Integration Tests' in testing_types:
                print("‚úÖ Application testing features properly configured")
            else:
                print("‚ùå Application testing features missing")
                return False
            
            return True
        else:
            print(f"‚ùå Capabilities endpoint failed: {response.status_code}")
            return False
    except Exception as e:
        print(f"‚ùå Capabilities endpoint failed: {e}")
        return False

def test_application_testing_module():
    """Test the application testing module."""
    print("\nüîç Testing application testing module...")
    try:
        # Import the module
        from modules.app_testing import ApplicationTestingModule
        
        # Initialize the module
        testing_module = ApplicationTestingModule()
        print("‚úÖ Application testing module imported and initialized")
        
        # Test project analysis
        current_dir = os.path.dirname(os.path.abspath(__file__))
        analysis = testing_module.analyze_project_structure(current_dir)
        
        if analysis and 'project_type' in analysis:
            print(f"‚úÖ Project analysis successful: {analysis['project_type']}")
            print(f"   Languages: {analysis.get('languages', [])}")
            print(f"   Frameworks: {analysis.get('frameworks', [])}")
            print(f"   Has tests: {analysis.get('has_tests', False)}")
        else:
            print("‚ùå Project analysis failed")
            return False
        
        return True
    except Exception as e:
        print(f"‚ùå Application testing module test failed: {e}")
        return False

def test_llm_finetuning_module():
    """Test the LLM fine-tuning module (without API key)."""
    print("\nüîç Testing LLM fine-tuning module...")
    try:
        # Test module import
        from modules.llm_finetuning import LLMFineTuningModule
        print("‚úÖ LLM fine-tuning module imported successfully")
        
        # Test initialization without API key (should fail gracefully)
        try:
            finetuning_module = LLMFineTuningModule()
            print("‚ùå LLM fine-tuning module should fail without API key")
            return False
        except ValueError as e:
            if "API key is required" in str(e):
                print("‚úÖ LLM fine-tuning module properly validates API key requirement")
            else:
                print(f"‚ùå Unexpected error: {e}")
                return False
        
        # Test data preparation functionality
        sample_data = [
            {
                "messages": [
                    {"role": "user", "content": "Hello"},
                    {"role": "assistant", "content": "Hi there!"}
                ]
            }
        ]
        
        # Create a temporary file for testing
        temp_file = "/tmp/test_training_data.jsonl"
        
        # Test with a mock API key
        os.environ['OPENAI_API_KEY'] = 'test-key-for-validation'
        try:
            finetuning_module = LLMFineTuningModule()
            prepared_file = finetuning_module.prepare_training_data(sample_data, temp_file)
            
            if os.path.exists(prepared_file):
                print("‚úÖ Training data preparation successful")
                
                # Test validation
                validation = finetuning_module.validate_training_data(prepared_file)
                if validation and 'valid' in validation:
                    print(f"‚úÖ Training data validation successful: {validation['valid']}")
                else:
                    print("‚ùå Training data validation failed")
                    return False
                
                # Clean up
                os.remove(prepared_file)
            else:
                print("‚ùå Training data preparation failed")
                return False
        finally:
            # Remove the test API key
            if 'OPENAI_API_KEY' in os.environ:
                del os.environ['OPENAI_API_KEY']
        
        return True
    except Exception as e:
        print(f"‚ùå LLM fine-tuning module test failed: {e}")
        return False

def test_api_endpoints():
    """Test the new API endpoints."""
    print("\nüîç Testing new API endpoints...")
    
    # Test application testing analyze endpoint
    try:
        current_dir = os.path.dirname(os.path.abspath(__file__))
        payload = {"project_path": current_dir}
        response = requests.post(
            'http://localhost:5001/api/testing/analyze',
            json=payload,
            timeout=10
        )
        
        if response.status_code == 200:
            data = response.json()
            if 'project_type' in data:
                print("‚úÖ Application testing analyze endpoint working")
            else:
                print("‚ùå Application testing analyze endpoint returned invalid data")
                return False
        else:
            print(f"‚ùå Application testing analyze endpoint failed: {response.status_code}")
            return False
    except Exception as e:
        print(f"‚ùå Application testing analyze endpoint failed: {e}")
        return False
    
    # Test LLM fine-tuning jobs endpoint (should work even without API key)
    try:
        response = requests.get('http://localhost:5001/api/llm/fine-tuning/jobs', timeout=5)
        if response.status_code in [200, 503]:  # 503 expected without API key
            print("‚úÖ LLM fine-tuning jobs endpoint accessible")
        else:
            print(f"‚ùå LLM fine-tuning jobs endpoint failed: {response.status_code}")
            return False
    except Exception as e:
        print(f"‚ùå LLM fine-tuning jobs endpoint failed: {e}")
        return False
    
    return True

def test_dashboard_accessibility():
    """Test if the React dashboard is accessible."""
    print("\nüîç Testing React dashboard accessibility...")
    try:
        response = requests.get('http://localhost:5173', timeout=5)
        if response.status_code == 200:
            print("‚úÖ React dashboard is accessible")
            return True
        else:
            print(f"‚ùå React dashboard failed: {response.status_code}")
            return False
    except Exception as e:
        print(f"‚ùå React dashboard failed: {e}")
        return False

def test_git_integration():
    """Test Git integration and repository status."""
    print("\nüîç Testing Git integration...")
    try:
        # Check if we're in a Git repository
        result = subprocess.run(['git', 'status'], capture_output=True, text=True)
        if result.returncode == 0:
            print("‚úÖ Git repository status accessible")
            
            # Check for recent commits
            result = subprocess.run(['git', 'log', '--oneline', '-5'], capture_output=True, text=True)
            if result.returncode == 0 and result.stdout:
                commits = result.stdout.strip().split('\n')
                print(f"‚úÖ Recent commits found: {len(commits)}")
                
                # Check for our new features in recent commits
                recent_commit = commits[0] if commits else ""
                if any(keyword in recent_commit.lower() for keyword in ['fine-tuning', 'testing', 'llm']):
                    print("‚úÖ Recent commits include new features")
                else:
                    print("‚ö†Ô∏è  Recent commits may not include new features")
            else:
                print("‚ùå Could not retrieve Git log")
                return False
        else:
            print("‚ùå Not in a Git repository or Git not accessible")
            return False
        
        return True
    except Exception as e:
        print(f"‚ùå Git integration test failed: {e}")
        return False

def run_comprehensive_validation():
    """Run all validation tests."""
    print("üöÄ Starting Comprehensive Validation of Enhanced AI Agent")
    print("=" * 60)
    
    tests = [
        ("Backend Health", test_backend_health),
        ("Capabilities Endpoint", test_capabilities_endpoint),
        ("Application Testing Module", test_application_testing_module),
        ("LLM Fine-tuning Module", test_llm_finetuning_module),
        ("API Endpoints", test_api_endpoints),
        ("Dashboard Accessibility", test_dashboard_accessibility),
        ("Git Integration", test_git_integration)
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        try:
            results[test_name] = test_func()
        except Exception as e:
            print(f"‚ùå {test_name} failed with exception: {e}")
            results[test_name] = False
    
    # Summary
    print("\n" + "=" * 60)
    print("üìä VALIDATION SUMMARY")
    print("=" * 60)
    
    passed = sum(1 for result in results.values() if result)
    total = len(results)
    
    for test_name, result in results.items():
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{test_name:<30} {status}")
    
    print("-" * 60)
    print(f"Total Tests: {total}")
    print(f"Passed: {passed}")
    print(f"Failed: {total - passed}")
    print(f"Success Rate: {(passed/total)*100:.1f}%")
    
    if passed == total:
        print("\nüéâ ALL TESTS PASSED! Enhanced AI Agent is fully functional.")
        return True
    else:
        print(f"\n‚ö†Ô∏è  {total - passed} test(s) failed. Please review the issues above.")
        return False

if __name__ == "__main__":
    success = run_comprehensive_validation()
    sys.exit(0 if success else 1)

